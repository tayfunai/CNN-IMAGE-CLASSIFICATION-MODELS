{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j5wa7AsROOS"
      },
      "source": [
        "## 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/tayfunai/CNN-BASED-CLASSIFICATION-MODELS.git"
      ],
      "metadata": {
        "id": "_E5ZkJP6rKqI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1P6n6CkBRBur"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qlBRyLdjRxpa"
      },
      "outputs": [],
      "source": [
        "\"\"\"In order force cuDNN to onyl use deterministic\n",
        "algorithms which might be useful to produce\n",
        "reproducible results we need following code\"\"\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLk-CDATSipS"
      },
      "source": [
        "## 2. Model settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wNe2ISQuSciF"
      },
      "outputs": [],
      "source": [
        "# Setting a random seed\n",
        "def set_all_seeds(seed):\n",
        "  os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lUyWFb43TL8h"
      },
      "outputs": [],
      "source": [
        "# Setting cuDNN and PyTorch algorithmic behavior to deterministic\n",
        "def set_deterministic():\n",
        "  if torch.cuda_is_available():\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "  torch.set_deterministic(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_Se7TTBrUx8Y"
      },
      "outputs": [],
      "source": [
        "#### SETTINGS ####\n",
        "\n",
        "# Hyperparameters\n",
        "RANDOM_SEED = 1\n",
        "LEARNING_RATE = 0.0001\n",
        "BATCH_SIZE = 256\n",
        "NUM_EPOCHS = 40\n",
        "\n",
        "# Architecture\n",
        "NUM_CLASSES=10\n",
        "\n",
        "#Other\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
        "\n",
        "set_all_seeds(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P2s1OeBBV6SR"
      },
      "outputs": [],
      "source": [
        "# Deterministic behavior not yet supported by AdaptiveAvgPool2d\n",
        "#set_deterministic()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_XssaqPV8wk"
      },
      "source": [
        "## 3. Import utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KGLJlElkV7n1"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# In order to import modules from a directory one level up.\n",
        "\n",
        "sys.path.append('/content/CNN-BASED-CLASSIFICATION-MODELS/helper_functions')\n",
        "sys.path.insert(0, \"..\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BaYA9Jruhf5I"
      },
      "outputs": [],
      "source": [
        "from helper_evaluate import compute_accuracy, compute_epoch_loss\n",
        "\n",
        "from helper_data import *\n",
        "\n",
        "from helper_train import *\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO1prwxOCId6"
      },
      "source": [
        "## 4. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MRBQ5-WiEVTU",
        "outputId": "8ea5239f-c55d-4f3a-9e28-55152e474835",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "slQrBxYICQHl"
      },
      "outputs": [],
      "source": [
        "## Set random seed##\n",
        "set_all_seeds(RANDOM_SEED)\n",
        "\n",
        "### Dataset ###\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((70,70)),\n",
        "    transforms.RandomCrop((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((70,70)),\n",
        "    transforms.CenterCrop((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_loader, valid_loader, test_loader = get_dataloaders_cifar10(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=2,\n",
        "    train_transforms=train_transforms,\n",
        "    test_transforms=test_transforms,\n",
        "    validation_fraction=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7pSDwZDkDKEH",
        "outputId": "05494c22-0456-4a80-fd96-32c2345f9bb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set:\n",
            "\n",
            "Image batch demensions: torch.Size([256, 3, 64, 64])\n",
            "Image label dimensions: torch.Size([256])\n",
            "tensor([0, 2, 3, 5, 4, 8, 9, 6, 9, 7])\n",
            "\n",
            "Validation set:\n",
            "Image batch demensions: torch.Size([256, 3, 64, 64])\n",
            "Image label dimensions: torch.Size([256])\n",
            "tensor([6, 9, 3, 5, 7, 3, 4, 1, 8, 0])\n",
            "\n",
            "Testing set:\n",
            "Image batch demensions: torch.Size([256, 3, 64, 64])\n",
            "Image label dimensions: torch.Size([256])\n",
            "tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1])\n"
          ]
        }
      ],
      "source": [
        "# Checking the dataset\n",
        "print(f\"Training set:\\n\")\n",
        "for images, labels in train_loader:\n",
        "  print(f\"Image batch demensions: {images.size()}\")\n",
        "  print(f\"Image label dimensions: {labels.size()}\")\n",
        "  print(labels[:10])\n",
        "  break\n",
        "\n",
        "# Checking the validation set\n",
        "print(f\"\\nValidation set:\")\n",
        "for images, labels in valid_loader:\n",
        "  print(f\"Image batch demensions: {images.size()}\")\n",
        "  print(f\"Image label dimensions: {labels.size()}\")\n",
        "  print(labels[:10])\n",
        "  break\n",
        "\n",
        "# Checking the testing set\n",
        "print(f\"\\nTesting set:\")\n",
        "for images, labels in test_loader:\n",
        "  print(f\"Image batch demensions: {images.size()}\")\n",
        "  print(f\"Image label dimensions: {labels.size()}\")\n",
        "  print(labels[:10])\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdbDLKuUGjWU"
      },
      "source": [
        "## 5. Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wgZ-tsHuJO9d"
      },
      "outputs": [],
      "source": [
        "### MODEL ###\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "    super(AlexNet, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "      nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\n",
        "      nn.Conv2d(64, 192, kernel_size=5, padding=2, groups=2),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\n",
        "      nn.Conv2d(192, 384, kernel_size=3, padding=1, groups=2),\n",
        "      nn.ReLU(inplace=True),\n",
        "\n",
        "      nn.Conv2d(384, 256, kernel_size=3, padding=1, groups=2),\n",
        "      nn.ReLU(inplace=True),\n",
        "\n",
        "      nn.Conv2d(256, 256, kernel_size=3, padding=1, groups=2),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "    )\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(256*6*6, 4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(4096, num_classes),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = x.view(x.size(0), 256*6*6)\n",
        "    logits = self.classifier(x)\n",
        "    probas = F.softmax(logits, dim=1)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YstKF2dAMfae"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(RANDOM_SEED)\n",
        "model = AlexNet(NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Model training"
      ],
      "metadata": {
        "id": "98Qrl1YlsTpw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IChyTU2qM0Lh",
        "outputId": "28fe765c-786e-44a6-9cb0-326b21a03dec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/040 | Batch 0000/0175 | Loss: 2.3021\n",
            "Epoch: 001/040 | Batch 0050/0175 | Loss: 2.0390\n",
            "Epoch: 001/040 | Batch 0100/0175 | Loss: 1.8914\n",
            "Epoch: 001/040 | Batch 0150/0175 | Loss: 1.8856\n",
            "***Epoch: 001/040 | Train. Acc.: 32.917% | Loss: 1.734\n",
            "***Epoch: 001/040 | Valid. Acc.: 33.540% | Loss: 1.707\n",
            "Time elapsed: 0.96 min\n",
            "Epoch: 002/040 | Batch 0000/0175 | Loss: 1.6837\n",
            "Epoch: 002/040 | Batch 0050/0175 | Loss: 1.6665\n",
            "Epoch: 002/040 | Batch 0100/0175 | Loss: 1.6248\n",
            "Epoch: 002/040 | Batch 0150/0175 | Loss: 1.5204\n",
            "***Epoch: 002/040 | Train. Acc.: 40.850% | Loss: 1.572\n",
            "***Epoch: 002/040 | Valid. Acc.: 41.540% | Loss: 1.556\n",
            "Time elapsed: 1.85 min\n",
            "Epoch: 003/040 | Batch 0000/0175 | Loss: 1.5176\n",
            "Epoch: 003/040 | Batch 0050/0175 | Loss: 1.6019\n",
            "Epoch: 003/040 | Batch 0100/0175 | Loss: 1.4632\n",
            "Epoch: 003/040 | Batch 0150/0175 | Loss: 1.2862\n",
            "***Epoch: 003/040 | Train. Acc.: 48.143% | Loss: 1.388\n",
            "***Epoch: 003/040 | Valid. Acc.: 48.320% | Loss: 1.385\n",
            "Time elapsed: 2.73 min\n",
            "Epoch: 004/040 | Batch 0000/0175 | Loss: 1.3607\n",
            "Epoch: 004/040 | Batch 0050/0175 | Loss: 1.3944\n",
            "Epoch: 004/040 | Batch 0100/0175 | Loss: 1.4087\n",
            "Epoch: 004/040 | Batch 0150/0175 | Loss: 1.3798\n",
            "***Epoch: 004/040 | Train. Acc.: 50.114% | Loss: 1.346\n",
            "***Epoch: 004/040 | Valid. Acc.: 50.780% | Loss: 1.339\n",
            "Time elapsed: 3.61 min\n",
            "Epoch: 005/040 | Batch 0000/0175 | Loss: 1.3042\n",
            "Epoch: 005/040 | Batch 0050/0175 | Loss: 1.2669\n",
            "Epoch: 005/040 | Batch 0100/0175 | Loss: 1.2392\n",
            "Epoch: 005/040 | Batch 0150/0175 | Loss: 1.2681\n",
            "***Epoch: 005/040 | Train. Acc.: 53.621% | Loss: 1.261\n",
            "***Epoch: 005/040 | Valid. Acc.: 54.680% | Loss: 1.240\n",
            "Time elapsed: 4.51 min\n",
            "Epoch: 006/040 | Batch 0000/0175 | Loss: 1.2688\n",
            "Epoch: 006/040 | Batch 0050/0175 | Loss: 1.2686\n",
            "Epoch: 006/040 | Batch 0100/0175 | Loss: 1.2055\n",
            "Epoch: 006/040 | Batch 0150/0175 | Loss: 1.1208\n",
            "***Epoch: 006/040 | Train. Acc.: 57.721% | Loss: 1.151\n",
            "***Epoch: 006/040 | Valid. Acc.: 58.180% | Loss: 1.156\n",
            "Time elapsed: 5.41 min\n",
            "Epoch: 007/040 | Batch 0000/0175 | Loss: 1.1161\n",
            "Epoch: 007/040 | Batch 0050/0175 | Loss: 1.1723\n",
            "Epoch: 007/040 | Batch 0100/0175 | Loss: 1.1882\n",
            "Epoch: 007/040 | Batch 0150/0175 | Loss: 1.3306\n",
            "***Epoch: 007/040 | Train. Acc.: 58.933% | Loss: 1.134\n",
            "***Epoch: 007/040 | Valid. Acc.: 59.040% | Loss: 1.133\n",
            "Time elapsed: 6.30 min\n",
            "Epoch: 008/040 | Batch 0000/0175 | Loss: 1.0558\n",
            "Epoch: 008/040 | Batch 0050/0175 | Loss: 1.1353\n",
            "Epoch: 008/040 | Batch 0100/0175 | Loss: 1.0678\n",
            "Epoch: 008/040 | Batch 0150/0175 | Loss: 1.0807\n",
            "***Epoch: 008/040 | Train. Acc.: 58.942% | Loss: 1.149\n",
            "***Epoch: 008/040 | Valid. Acc.: 58.580% | Loss: 1.168\n",
            "Time elapsed: 7.18 min\n",
            "Epoch: 009/040 | Batch 0000/0175 | Loss: 1.2266\n",
            "Epoch: 009/040 | Batch 0050/0175 | Loss: 1.0750\n",
            "Epoch: 009/040 | Batch 0100/0175 | Loss: 1.1740\n",
            "Epoch: 009/040 | Batch 0150/0175 | Loss: 1.0020\n",
            "***Epoch: 009/040 | Train. Acc.: 60.683% | Loss: 1.083\n",
            "***Epoch: 009/040 | Valid. Acc.: 59.620% | Loss: 1.103\n",
            "Time elapsed: 8.07 min\n",
            "Epoch: 010/040 | Batch 0000/0175 | Loss: 1.0745\n",
            "Epoch: 010/040 | Batch 0050/0175 | Loss: 1.0531\n",
            "Epoch: 010/040 | Batch 0100/0175 | Loss: 0.9789\n",
            "Epoch: 010/040 | Batch 0150/0175 | Loss: 1.2133\n",
            "***Epoch: 010/040 | Train. Acc.: 64.645% | Loss: 0.974\n",
            "***Epoch: 010/040 | Valid. Acc.: 63.340% | Loss: 1.018\n",
            "Time elapsed: 8.96 min\n",
            "Epoch: 011/040 | Batch 0000/0175 | Loss: 1.0149\n",
            "Epoch: 011/040 | Batch 0050/0175 | Loss: 1.0811\n",
            "Epoch: 011/040 | Batch 0100/0175 | Loss: 0.9565\n",
            "Epoch: 011/040 | Batch 0150/0175 | Loss: 1.0607\n",
            "***Epoch: 011/040 | Train. Acc.: 65.491% | Loss: 0.960\n",
            "***Epoch: 011/040 | Valid. Acc.: 64.340% | Loss: 1.012\n",
            "Time elapsed: 9.85 min\n",
            "Epoch: 012/040 | Batch 0000/0175 | Loss: 1.0149\n",
            "Epoch: 012/040 | Batch 0050/0175 | Loss: 1.0555\n",
            "Epoch: 012/040 | Batch 0100/0175 | Loss: 0.8594\n",
            "Epoch: 012/040 | Batch 0150/0175 | Loss: 0.9643\n",
            "***Epoch: 012/040 | Train. Acc.: 66.451% | Loss: 0.925\n",
            "***Epoch: 012/040 | Valid. Acc.: 64.780% | Loss: 1.002\n",
            "Time elapsed: 10.75 min\n",
            "Epoch: 013/040 | Batch 0000/0175 | Loss: 0.8943\n",
            "Epoch: 013/040 | Batch 0050/0175 | Loss: 0.9128\n",
            "Epoch: 013/040 | Batch 0100/0175 | Loss: 1.0230\n",
            "Epoch: 013/040 | Batch 0150/0175 | Loss: 1.0029\n",
            "***Epoch: 013/040 | Train. Acc.: 68.455% | Loss: 0.877\n",
            "***Epoch: 013/040 | Valid. Acc.: 66.380% | Loss: 0.952\n",
            "Time elapsed: 11.64 min\n",
            "Epoch: 014/040 | Batch 0000/0175 | Loss: 0.8482\n",
            "Epoch: 014/040 | Batch 0050/0175 | Loss: 0.9762\n",
            "Epoch: 014/040 | Batch 0100/0175 | Loss: 0.9270\n",
            "Epoch: 014/040 | Batch 0150/0175 | Loss: 0.8943\n",
            "***Epoch: 014/040 | Train. Acc.: 70.424% | Loss: 0.832\n",
            "***Epoch: 014/040 | Valid. Acc.: 67.620% | Loss: 0.929\n",
            "Time elapsed: 12.52 min\n",
            "Epoch: 015/040 | Batch 0000/0175 | Loss: 0.9535\n",
            "Epoch: 015/040 | Batch 0050/0175 | Loss: 0.8244\n",
            "Epoch: 015/040 | Batch 0100/0175 | Loss: 0.7830\n",
            "Epoch: 015/040 | Batch 0150/0175 | Loss: 0.8044\n",
            "***Epoch: 015/040 | Train. Acc.: 69.422% | Loss: 0.853\n",
            "***Epoch: 015/040 | Valid. Acc.: 66.280% | Loss: 0.966\n",
            "Time elapsed: 13.41 min\n",
            "Epoch: 016/040 | Batch 0000/0175 | Loss: 0.9202\n",
            "Epoch: 016/040 | Batch 0050/0175 | Loss: 0.9646\n",
            "Epoch: 016/040 | Batch 0100/0175 | Loss: 0.8713\n",
            "Epoch: 016/040 | Batch 0150/0175 | Loss: 0.8250\n",
            "***Epoch: 016/040 | Train. Acc.: 70.047% | Loss: 0.846\n",
            "***Epoch: 016/040 | Valid. Acc.: 66.220% | Loss: 0.955\n",
            "Time elapsed: 14.31 min\n",
            "Epoch: 017/040 | Batch 0000/0175 | Loss: 0.8466\n",
            "Epoch: 017/040 | Batch 0050/0175 | Loss: 0.8638\n",
            "Epoch: 017/040 | Batch 0100/0175 | Loss: 0.7317\n",
            "Epoch: 017/040 | Batch 0150/0175 | Loss: 1.0237\n",
            "***Epoch: 017/040 | Train. Acc.: 72.627% | Loss: 0.774\n",
            "***Epoch: 017/040 | Valid. Acc.: 68.020% | Loss: 0.925\n",
            "Time elapsed: 15.20 min\n",
            "Epoch: 018/040 | Batch 0000/0175 | Loss: 0.7874\n",
            "Epoch: 018/040 | Batch 0050/0175 | Loss: 0.8139\n",
            "Epoch: 018/040 | Batch 0100/0175 | Loss: 0.8466\n",
            "Epoch: 018/040 | Batch 0150/0175 | Loss: 0.8492\n",
            "***Epoch: 018/040 | Train. Acc.: 73.973% | Loss: 0.744\n",
            "***Epoch: 018/040 | Valid. Acc.: 68.920% | Loss: 0.895\n",
            "Time elapsed: 16.09 min\n",
            "Epoch: 019/040 | Batch 0000/0175 | Loss: 0.8022\n",
            "Epoch: 019/040 | Batch 0050/0175 | Loss: 0.7180\n",
            "Epoch: 019/040 | Batch 0100/0175 | Loss: 0.7955\n",
            "Epoch: 019/040 | Batch 0150/0175 | Loss: 0.8377\n",
            "***Epoch: 019/040 | Train. Acc.: 75.083% | Loss: 0.706\n",
            "***Epoch: 019/040 | Valid. Acc.: 69.500% | Loss: 0.892\n",
            "Time elapsed: 16.97 min\n",
            "Epoch: 020/040 | Batch 0000/0175 | Loss: 0.7129\n",
            "Epoch: 020/040 | Batch 0050/0175 | Loss: 0.8321\n",
            "Epoch: 020/040 | Batch 0100/0175 | Loss: 0.7406\n",
            "Epoch: 020/040 | Batch 0150/0175 | Loss: 0.8022\n",
            "***Epoch: 020/040 | Train. Acc.: 75.940% | Loss: 0.676\n",
            "***Epoch: 020/040 | Valid. Acc.: 69.940% | Loss: 0.872\n",
            "Time elapsed: 17.86 min\n",
            "Epoch: 021/040 | Batch 0000/0175 | Loss: 0.7002\n",
            "Epoch: 021/040 | Batch 0050/0175 | Loss: 0.7048\n",
            "Epoch: 021/040 | Batch 0100/0175 | Loss: 0.6758\n",
            "Epoch: 021/040 | Batch 0150/0175 | Loss: 0.8150\n",
            "***Epoch: 021/040 | Train. Acc.: 75.699% | Loss: 0.685\n",
            "***Epoch: 021/040 | Valid. Acc.: 69.580% | Loss: 0.887\n",
            "Time elapsed: 18.77 min\n",
            "Epoch: 022/040 | Batch 0000/0175 | Loss: 0.7320\n",
            "Epoch: 022/040 | Batch 0050/0175 | Loss: 0.6388\n",
            "Epoch: 022/040 | Batch 0100/0175 | Loss: 0.6263\n",
            "Epoch: 022/040 | Batch 0150/0175 | Loss: 0.6756\n",
            "***Epoch: 022/040 | Train. Acc.: 77.440% | Loss: 0.643\n",
            "***Epoch: 022/040 | Valid. Acc.: 70.160% | Loss: 0.878\n",
            "Time elapsed: 19.66 min\n",
            "Epoch: 023/040 | Batch 0000/0175 | Loss: 0.7086\n",
            "Epoch: 023/040 | Batch 0050/0175 | Loss: 0.5649\n",
            "Epoch: 023/040 | Batch 0100/0175 | Loss: 0.7574\n",
            "Epoch: 023/040 | Batch 0150/0175 | Loss: 0.6838\n",
            "***Epoch: 023/040 | Train. Acc.: 79.406% | Loss: 0.587\n",
            "***Epoch: 023/040 | Valid. Acc.: 71.540% | Loss: 0.847\n",
            "Time elapsed: 20.54 min\n",
            "Epoch: 024/040 | Batch 0000/0175 | Loss: 0.5580\n",
            "Epoch: 024/040 | Batch 0050/0175 | Loss: 0.6834\n",
            "Epoch: 024/040 | Batch 0100/0175 | Loss: 0.5598\n",
            "Epoch: 024/040 | Batch 0150/0175 | Loss: 0.6000\n",
            "***Epoch: 024/040 | Train. Acc.: 79.306% | Loss: 0.591\n",
            "***Epoch: 024/040 | Valid. Acc.: 70.400% | Loss: 0.870\n",
            "Time elapsed: 21.42 min\n",
            "Epoch: 025/040 | Batch 0000/0175 | Loss: 0.6278\n",
            "Epoch: 025/040 | Batch 0050/0175 | Loss: 0.5846\n",
            "Epoch: 025/040 | Batch 0100/0175 | Loss: 0.5988\n",
            "Epoch: 025/040 | Batch 0150/0175 | Loss: 0.6806\n",
            "***Epoch: 025/040 | Train. Acc.: 80.022% | Loss: 0.578\n",
            "***Epoch: 025/040 | Valid. Acc.: 70.780% | Loss: 0.860\n",
            "Time elapsed: 22.32 min\n",
            "Epoch: 026/040 | Batch 0000/0175 | Loss: 0.6620\n",
            "Epoch: 026/040 | Batch 0050/0175 | Loss: 0.6389\n",
            "Epoch: 026/040 | Batch 0100/0175 | Loss: 0.6643\n",
            "Epoch: 026/040 | Batch 0150/0175 | Loss: 0.5626\n",
            "***Epoch: 026/040 | Train. Acc.: 81.565% | Loss: 0.523\n",
            "***Epoch: 026/040 | Valid. Acc.: 72.360% | Loss: 0.844\n",
            "Time elapsed: 23.22 min\n",
            "Epoch: 027/040 | Batch 0000/0175 | Loss: 0.5112\n",
            "Epoch: 027/040 | Batch 0050/0175 | Loss: 0.5579\n",
            "Epoch: 027/040 | Batch 0100/0175 | Loss: 0.6548\n",
            "Epoch: 027/040 | Batch 0150/0175 | Loss: 0.4701\n",
            "***Epoch: 027/040 | Train. Acc.: 80.855% | Loss: 0.544\n",
            "***Epoch: 027/040 | Valid. Acc.: 70.920% | Loss: 0.889\n",
            "Time elapsed: 24.11 min\n",
            "Epoch: 028/040 | Batch 0000/0175 | Loss: 0.4991\n",
            "Epoch: 028/040 | Batch 0050/0175 | Loss: 0.5016\n",
            "Epoch: 028/040 | Batch 0100/0175 | Loss: 0.5291\n",
            "Epoch: 028/040 | Batch 0150/0175 | Loss: 0.5704\n",
            "***Epoch: 028/040 | Train. Acc.: 82.842% | Loss: 0.493\n",
            "***Epoch: 028/040 | Valid. Acc.: 71.200% | Loss: 0.873\n",
            "Time elapsed: 24.99 min\n",
            "Epoch: 029/040 | Batch 0000/0175 | Loss: 0.5912\n",
            "Epoch: 029/040 | Batch 0050/0175 | Loss: 0.6490\n",
            "Epoch: 029/040 | Batch 0100/0175 | Loss: 0.6406\n",
            "Epoch: 029/040 | Batch 0150/0175 | Loss: 0.4002\n",
            "***Epoch: 029/040 | Train. Acc.: 83.299% | Loss: 0.481\n",
            "***Epoch: 029/040 | Valid. Acc.: 71.900% | Loss: 0.873\n",
            "Time elapsed: 25.88 min\n",
            "Epoch: 030/040 | Batch 0000/0175 | Loss: 0.4956\n",
            "Epoch: 030/040 | Batch 0050/0175 | Loss: 0.4457\n",
            "Epoch: 030/040 | Batch 0100/0175 | Loss: 0.4946\n",
            "Epoch: 030/040 | Batch 0150/0175 | Loss: 0.6139\n"
          ]
        }
      ],
      "source": [
        "log_dict = train_classifier_simple_v1(num_epochs=NUM_EPOCHS,\n",
        "                                      model=model,\n",
        "                                      optimizer=optimizer,\n",
        "                                      device=DEVICE,\n",
        "                                      train_loader=train_loader,\n",
        "                                      valid_loader=valid_loader,\n",
        "                                      logging_interval=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model Evaluation"
      ],
      "metadata": {
        "id": "VLopqd-E7tQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "id": "84zm948KsqcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = log_dict['train_loss_per_batch']\n",
        "\n",
        "plt.plot(loss_list, label='Minibatch loss')\n",
        "plt.plot(np.convolve(loss_list,\n",
        "                     np.ones(200,)/200, mode='valid'),\n",
        "         label='Running average')\n",
        "\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.xlabel('Iteration')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gIoNuwWA7zke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.arange(1, NUM_EPOCHS+1), log_dict['train_acc_per_epoch'], label='Training')\n",
        "plt.plot(np.arange(1, NUM_EPOCHS+1), log_dict['valid_acc_per_epoch'], label='Validation')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O5p88P3z76zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.set_grad_enabled(False):\n",
        "\n",
        "    train_acc = compute_accuracy(model=model,\n",
        "                                 data_loader=test_loader,\n",
        "                                 device=DEVICE)\n",
        "\n",
        "    test_acc = compute_accuracy(model=model,\n",
        "                                data_loader=test_loader,\n",
        "                                device=DEVICE)\n",
        "\n",
        "    valid_acc = compute_accuracy(model=model,\n",
        "                                 data_loader=valid_loader,\n",
        "                                 device=DEVICE)\n",
        "\n",
        "\n",
        "print(f'Train ACC: {valid_acc:.2f}%')\n",
        "print(f'Validation ACC: {valid_acc:.2f}%')\n",
        "print(f'Test ACC: {test_acc:.2f}%')\n"
      ],
      "metadata": {
        "id": "SHukQm74E2gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ZosB9PTE8A6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}